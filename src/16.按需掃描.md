# 16. Scanning on Demand 按需掃描

> Literature is idiosyncratic arrangements in horizontal lines in only twenty-six phonetic symbols, ten Arabic numbers, and about eight punctuation marks.
>
> ——  Kurt Vonnegut, *Like Shaking Hands With God: A Conversation about Writing*

文學就是26個字母、10個阿拉伯數字和大概8個標點符號的水平排列。（馮尼古特《像與上帝握手：關於寫作的談話》）

> Our second interpreter, clox, has three phases—scanner, compiler, and virtual machine. A data structure joins each pair of phases. Tokens flow from scanner to compiler, and chunks of bytecode from compiler to VM. We began our implementation near the end with [chunks](http://www.craftinginterpreters.com/chunks-of-bytecode.html) and the [VM](http://www.craftinginterpreters.com/a-virtual-machine.html). Now, we’re going to hop back to the beginning and build a scanner that makes tokens. In the [next chapter](http://www.craftinginterpreters.com/compiling-expressions.html), we’ll tie the two ends together with our bytecode compiler.
>

我們的第二個解釋器clox分為三個階段——掃描器、編譯器和虛擬機。每兩個階段之間有一個數據結構進行銜接。詞法標識從掃描器流入編譯器，字節碼塊從編譯器流向虛擬機。我們是從尾部開始先實現了字節碼塊和虛擬機。現在，我們要回到起點，構建一個生成詞法標識的掃描器。在下一章中，我們將用字節碼編譯器將這兩部分連接起來。

![Source code → scanner → tokens → compiler → bytecode chunk → VM.](16.按需掃描/pipeline.png)

> I’ll admit, this is not the most exciting chapter in the book. With two implementations of the same language, there’s bound to be some redundancy. I did sneak in a few interesting differences compared to jlox’s scanner. Read on to see what they are.
>

我承認，這並不是書中最精彩的一章。對於同一種語言的兩個實現，肯定會有一些冗餘。與jlox的掃描器相比，我確實添加了一些有趣的差異點。往下讀，看看它們是什麼。

## 16 . 1 Spinning Up the Interpreter

> Now that we’re building the front end, we can get clox running like a real interpreter. No more hand-authored chunks of bytecode. It’s time for a REPL and script loading. Tear out most of the code in `main()` and replace it with:

現在我們正在構建前端，我們可以讓clox像一個真正的解釋器一樣運行。不需要再手動編寫字節碼塊。現在是時候實現REPL和腳本加載了。刪除`main()`方法中的大部分代碼，替換成：

*<u>main.c，在 main()方法中替換26行：</u>*

```c
int main(int argc, const char* argv[]) {
  initVM();
  // 替換部分開始
  if (argc == 1) {
    repl();
  } else if (argc == 2) {
    runFile(argv[1]);
  } else {
    fprintf(stderr, "Usage: clox [path]\n");
    exit(64);
  }
  
  freeVM();
  // 替換部分結束
  return 0;
}
```

> If you pass no arguments to the executable, you are dropped into the REPL. A single command line argument is understood to be the path to a script to run.

如果你沒有向可執行文件傳遞任何參數，就會進入REPL。如果傳入一個參數，就將其當做要運行的腳本的路徑[^1]。

> We’ll need a few system headers, so let’s get them all out of the way.

我們需要一些系統頭文件，所以把它們都列出來。

*<u>main.c，在文件頂部添加：</u>*

```c
// 新增部分開始
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
// 新增部分結束
#include "common.h"
```

> Next, we get the REPL up and REPL-ing.

接下來，我們啓動REPL並運行。

*<u>main.c，添加代碼：</u>*

```c
#include "vm.h"
// 新增部分開始
static void repl() {
  char line[1024];
  for (;;) {
    printf("> ");

    if (!fgets(line, sizeof(line), stdin)) {
      printf("\n");
      break;
    }

    interpret(line);
  }
}
// 新增部分結束
```

> A quality REPL handles input that spans multiple lines gracefully and doesn’t have a hardcoded line length limit. This REPL here is a little more, ahem, austere, but it’s fine for our purposes.

一個高質量的REPL可以優雅地處理多行的輸入，並且沒有硬編碼的行長度限制。這裏的REPL有點……簡樸，但足以滿足我們的需求。

> The real work happens in `interpret()`. We’ll get to that soon, but first let’s take care of loading scripts.

真正的工作發生在`interpret()`中。我們很快會講到這個問題，但首先讓我們來看看如何加載腳本。

*<u>main.c，在repl()方法後添加：</u>*

```c
// 新增部分開始
static void runFile(const char* path) {
  char* source = readFile(path);
  InterpretResult result = interpret(source);
  free(source); 

  if (result == INTERPRET_COMPILE_ERROR) exit(65);
  if (result == INTERPRET_RUNTIME_ERROR) exit(70);
}
// 新增部分結束
```

> We read the file and execute the resulting string of Lox source code. Then, based on the result of that, we set the exit code appropriately because we’re scrupulous tool builders and care about little details like that.

我們讀取文件並執行生成的Lox源碼字符串。然後，根據其結果，我們適當地設置退出碼，因為我們是嚴謹的工具製作者，並且關心這樣的小細節。

> We also need to free the source code string because `readFile()` dynamically allocates it and passes ownership to its caller. That function looks like this:

我們還需要釋放源代碼字符串，因為`readFile()`會動態地分配內存，並將所有權傳遞給它的調用者[^2]。這個函數看起來像這樣：

*<u>main.c，在 repl()方法後添加代碼：</u>*

```c
static char* readFile(const char* path) {
  FILE* file = fopen(path, "rb");

  fseek(file, 0L, SEEK_END);
  size_t fileSize = ftell(file);
  rewind(file);

  char* buffer = (char*)malloc(fileSize + 1);
  size_t bytesRead = fread(buffer, sizeof(char), fileSize, file);
  buffer[bytesRead] = '\0';

  fclose(file);
  return buffer;
}
```

> Like a lot of C code, it takes more effort than it seems like it should, especially for a language expressly designed for operating systems. The difficult part is that we want to allocate a big enough string to read the whole file, but we don’t know how big the file is until we’ve read it.

像很多C語言代碼一樣，它所花費的精力比看起來要多，尤其是對於一門專為操作系統而設計的語言而言。困難的地方在於，我們想分配一個足以讀取整個文件的字符串，但是我們在讀取文件之前並不知道它有多大。

> The code here is the classic trick to solve that. We open the file, but before reading it, we seek to the very end using `fseek()`. Then we call `ftell()` which tells us how many bytes we are from the start of the file. Since we seeked (sought?) to the end, that’s the size. We rewind back to the beginning, allocate a string of that size, and read the whole file in a single batch.

這裏的代碼是解決這個問題的經典技巧。我們打開文件，但是在讀之前，先通過`fseek()`尋找到文件的最末端。接下來我們調用`ftell()`，它會告訴我們裏文件起始點有多少字節。既然我們定位到了最末端，那它就是文件大小[^3]。我們退回到起始位置，分配一個相同大小的字符串，然後一次性讀取整個文件。

> So we’re done, right? Not quite. These function calls, like most calls in the C standard library, can fail. If this were Java, the failures would be thrown as exceptions and automatically unwind the stack so we wouldn’t *really* need to handle them. In C, if we don’t check for them, they silently get ignored.

這樣就完成了嗎？不完全是。這些函數調用，像C語言標準庫中的大多數調用一樣，可能會失敗。如果是在Java中，這些失敗會被當做異常拋出，並自動清除堆棧，所以我們實際上並不需要處理它們。但在C語言中，如果我們不檢查，它們就會被忽略。

> This isn’t really a book on good C programming practice, but I hate to encourage bad style, so let’s go ahead and handle the errors. It’s good for us, like eating our vegetables or flossing.

這並不是一本關於良好C語言編程實踐的書，但我討厭鼓勵糟糕的編程風格，所以讓我們繼續處理這些錯誤。這對我們有好處，就像吃蔬菜或使用牙線清潔牙齒一樣。

> Fortunately, we don’t need to do anything particularly clever if a failure occurs. If we can’t correctly read the user’s script, all we can really do is tell the user and exit the interpreter gracefully. First of all, we might fail to open the file.

幸運地是，如果發生故障，我們不需要特別聰明的做法。如果我們不能正確地讀取用户的腳本，我們真正能做的就是告訴用户並優雅地退出解釋器。首先，我們可能無法打開文件。

*<u>main.c，在readFile()方法中添加代碼：</u>*

```c
  FILE* file = fopen(path, "rb");
  // 新增部分開始
  if (file == NULL) {
    fprintf(stderr, "Could not open file \"%s\".\n", path);
    exit(74);
  }
  // 新增部分結束
  fseek(file, 0L, SEEK_END);
```

> This can happen if the file doesn’t exist or the user doesn’t have access to it. It’s pretty common—people mistype paths all the time.

如果文件不存在或用户沒有訪問權限，就會發生這種情況。這是很常見的——人們經常會輸入錯誤的路徑。

> This failure is much rarer:

下面這種錯誤要少見得多：

*<u>main.c，在readFile()方法中添加代碼：</u>*

```c
  char* buffer = (char*)malloc(fileSize + 1);
  // 新增部分開始
  if (buffer == NULL) {
    fprintf(stderr, "Not enough memory to read \"%s\".\n", path);
    exit(74);
  }
  // 新增部分結束
  size_t bytesRead = fread(buffer, sizeof(char), fileSize, file);
```

> If we can’t even allocate enough memory to read the Lox script, the user’s probably got bigger problems to worry about, but we should do our best to at least let them know.

如果我們甚至不能分配足夠的內存來讀取Lox腳本，那麼用户可能會有更大的問題需要擔心，但我們至少應該盡最大努力讓他們知道。

> Finally, the read itself may fail.

最後，讀取本身可能會失敗。

*<u>main.c，在readFile()方法中添加代碼：</u>*

```c
  size_t bytesRead = fread(buffer, sizeof(char), fileSize, file);
  // 新增部分開始
  if (bytesRead < fileSize) {
    fprintf(stderr, "Could not read file \"%s\".\n", path);
    exit(74);
  }
  // 新增部分結束
  buffer[bytesRead] = '\0';
```

> This is also unlikely. Actually, the calls to `fseek()`, `ftell()`, and `rewind()` could theoretically fail too, but let’s not go too far off in the weeds, shall we?

這也是不大可能發生的。實際上，`fseek()`, `ftell()`, 和`rewind()` 的調用在理論上也可能會失敗，但是我們不要太過深入，好嗎？

> ### 16 . 1 . 1 Opening the compilation pipeline

### 16.1.1 開啓編譯管道

> We’ve got ourselves a string of Lox source code, so now we’re ready to set up a pipeline to scan, compile, and execute it. It’s driven by `interpret()`. Right now, that function runs our old hardcoded test chunk. Let’s change it to something closer to its final incarnation.

我們已經得到了Lox源代碼字符串，所以現在我們準備建立一個管道來掃描、編譯和執行它。管道是由`interpret()`驅動的。現在，該函數運行的是舊的硬編碼測試字節碼塊。我們來把它改成更接近其最終形態的東西。

*<u>vm.h，函數interpret()中替換1行：</u>*

```c
void freeVM();
// 替換部分開始
InterpretResult interpret(const char* source);
// 替換部分結束
void push(Value value);
```

> Where before we passed in a Chunk, now we pass in the string of source code. Here’s the new implementation:

以前我們傳入一個字節碼塊，現在我們傳入的是源代碼的字符串。下面是新的實現：

*<u>vm.c，函數interpret()中替換4行：</u>*

```c
// 替換部分開始
InterpretResult interpret(const char* source) {
  compile(source);
  return INTERPRET_OK;
//  替換部分結束
}
```

> We won’t build the actual *compiler* yet in this chapter, but we can start laying out its structure. It lives in a new module.

在本章中，我們還不會構建真正的*編譯器*，但我們可以開始佈局它的結構。它存在於一個新的模塊中。

*<u>vm.c，添加代碼：</u>*

```c
#include "common.h"
// 新增部分開始
#include "compiler.h"
// 新增部分結束
#include "debug.h"
```

> For now, the one function in it is declared like so:

目前，其中有一個函數聲明如下：

*<u>compiler.h，創建新文件：</u>*

```c
#ifndef clox_compiler_h
#define clox_compiler_h

void compile(const char* source);

#endif
```

> That signature will change, but it gets us going.

這個簽名以後會變，但現在足以讓我們繼續工作。

> The first phase of compilation is scanning—the thing we’re doing in this chapter—so right now all the compiler does is set that up.

編譯的第一階段是掃描——即我們在本章中要做的事情——所以現在編譯器所做的就是設置掃描。

*<u>compiler.c，創建新文件：</u>*

```c
#include <stdio.h>

#include "common.h"
#include "compiler.h"
#include "scanner.h"

void compile(const char* source) {
  initScanner(source);
}
```

> This will also grow in later chapters, naturally.

當然，這在後面的章節中也會繼續擴展。

> ### 16 . 1 . 2 The scanner scans

### 16.1.2 掃描器掃描

> There are still a few more feet of scaffolding to stand up before we can start writing useful code. First, a new header:

在我們開始編寫實際有用的代碼之前，還有一些腳手架需要先搭建起來。首先，是一個新的頭文件：

*<u>scanner.h，創建新文件：</u>*

```c
#ifndef clox_scanner_h
#define clox_scanner_h

void initScanner(const char* source);

#endif
```

> And its corresponding implementation:

還有其對應的實現：

*<u>scanner.c，創建新文件：</u>*

```c
#include <stdio.h>
#include <string.h>

#include "common.h"
#include "scanner.h"

typedef struct {
  const char* start;
  const char* current;
  int line;
} Scanner;

Scanner scanner;
```

> As our scanner chews through the user’s source code, it tracks how far it’s gone. Like we did with the VM, we wrap that state in a struct and then create a single top-level module variable of that type so we don’t have to pass it around all of the various functions.

當我們的掃描器一點點處理用户的源代碼時，它會跟蹤自己已經走了多遠。就像我們在虛擬機中所做的那樣，我們將狀態封裝在一個結構體中，然後創建一個該類型的頂層模塊變量，這樣就不必在所有的函數之間傳遞它。

> There are surprisingly few fields. The `start` pointer marks the beginning of the current lexeme being scanned, and `current` points to the current character being looked at.

這裏的字段少得驚人。`start`指針標識正在被掃描的詞素的起點，而`current`指針指向當前正在查看的字符。

![The start and current fields pointing at 'print bacon;'. Start points at 'b' and current points at 'o'.](16.按需掃描/fields.png)

> We have a `line` field to track what line the current lexeme is on for error reporting. That’s it! We don’t even keep a pointer to the beginning of the source code string. The scanner works its way through the code once and is done after that.

我們還有一個`line`字段，用於跟蹤當前詞素在哪一行，以便進行錯誤報告。就是這樣！我們甚至沒有保留指向源代碼字符串起點的指針。掃描器只處理一遍代碼，然後就結束了。

> Since we have some state, we should initialize it.

因為我們有一些狀態，我們還應該初始化它。

*<u>scanner.c，在變量scanner後添加代碼：</u>*

```c
void initScanner(const char* source) {
  scanner.start = source;
  scanner.current = source;
  scanner.line = 1;
}
```

> We start at the very first character on the very first line, like a runner crouched at the starting line.

我們從第一行的第一個字符開始，就像一個運動員蹲在起跑線上。

> ## 16 . 2 A Token at a Time

## 16.2 一次一個標識

> In jlox, when the starting gun went off, the scanner raced ahead and eagerly scanned the whole program, returning a list of tokens. This would be a challenge in clox. We’d need some sort of growable array or list to store the tokens in. We’d need to manage allocating and freeing the tokens, and the collection itself. That’s a lot of code, and a lot of memory churn.

在jlox中，當發令槍響起時，掃描器飛快地前進，急切地掃描整個程序，並返回一個詞法標識序列。這在clox中有點困難。我們需要某種可增長的數組或列表來存儲標識。我們需要管理標識的分配和釋放，以及集合本身。這需要大量的代碼和大量的內存。

> At any point in time, the compiler needs only one or two tokens—remember our grammar requires only a single token of lookahead—so we don’t need to keep them *all* around at the same time. Instead, the simplest solution is to not scan a token until the compiler needs one. When the scanner provides one, it returns the token by value. It doesn’t need to dynamically allocate anything—it can just pass tokens around on the C stack.

在任何時間點，編譯器只需要一個或兩個詞法標識——記住我們的語法只需要前瞻一個詞法標識——所以我們不需要同時保留它們。相反，最簡單的解決方案是在編譯器需要標識的時候再去掃描。當掃描器提供一個標識時，它按值返回標識。它不需要動態分配任何東西——只需要在C棧上傳遞詞法標識即可。

> Unfortunately, we don’t have a compiler yet that can ask the scanner for tokens, so the scanner will just sit there doing nothing. To kick it into action, we’ll write some temporary code to drive it.

不巧的是，我們還沒有可以向掃描器請求詞法標識的編譯器，所以掃描器只能乾等着什麼也不做。為了讓它工作起來，我們要編寫一些臨時代碼來驅動它[^4]。

*<u>compiler.c，在compile()方法中添加代碼：</u>*

```c
  initScanner(source);
  // 新增部分開始
  int line = -1;
  for (;;) {
    Token token = scanToken();
    if (token.line != line) {
      printf("%4d ", token.line);
      line = token.line;
    } else {
      printf("   | ");
    }
    printf("%2d '%.*s'\n", token.type, token.length, token.start); 

    if (token.type == TOKEN_EOF) break;
  }
  // 新增部分結束
}
```

> This loops indefinitely. Each turn through the loop, it scans one token and prints it. When it reaches a special “end of file” token or an error, it stops. For example, if we run the interpreter on this program:

這個循環是無限的。每循環一次，它就會掃描一個詞法標識並打印出來。當它遇到特殊的“文件結束”標識或錯誤時，就會停止。例如，如果我們對下面的程序運行解釋器：

```c
print 1 + 2;
```

> It prints out:

就會打印出：

```c
   1 31 'print'
   | 21 '1'
   |  7 '+'
   | 21 '2'
   |  8 ';'
   2 39 ''
```

> The first column is the line number, the second is the numeric value of the token type, and then finally the lexeme. That last empty lexeme on line 2 is the EOF token.

第一列是行號，第二列是標識類型的數值，最後是詞素。第2行中最後一個空詞素就是EOF標識。

> The goal for the rest of the chapter is to make that blob of code work by implementing this key function:

本章其餘部分的目標就是通過實現下面這個關鍵函數，使這塊代碼能正常工作：

*<u>scanner.h，在initScanner()方法後添加：</u>*

```c
void initScanner(const char* source);
// 新增部分開始
Token scanToken();
// 新增部分結束
#endif
```

> Each call scans and returns the next token in the source code. A token looks like this:

該函數的每次調用都會掃描並返回源代碼中的下一個詞法標識。一個詞法標識結構如下：

*<u>scanner.h，添加代碼：</u>*

```c
#define clox_scanner_h
// 新增部分開始
typedef struct {
  TokenType type;
  const char* start;
  int length;
  int line;
} Token;
// 新增部分結束
void initScanner(const char* source);
```

> It’s pretty similar to jlox’s Token class. We have an enum identifying what type of token it is—number, identifier, `+` operator, etc. The enum is virtually identical to the one in jlox, so let’s just hammer out the whole thing.

它和jlox中的Token類很相似。我們用一個枚舉來標記它是什麼類型的詞法標識——數字、標識符、`+`運算符等等。這個枚舉與jlox中的枚舉幾乎完全相同，所以我們直接來敲定整個事情。

*<u>scanner.h，添加代碼：</u>*

```c
#ifndef clox_scanner_h
#define clox_scanner_h
// 新增部分開始
typedef enum {
  // Single-character tokens. 單字符詞法
  TOKEN_LEFT_PAREN, TOKEN_RIGHT_PAREN,
  TOKEN_LEFT_BRACE, TOKEN_RIGHT_BRACE,
  TOKEN_COMMA, TOKEN_DOT, TOKEN_MINUS, TOKEN_PLUS,
  TOKEN_SEMICOLON, TOKEN_SLASH, TOKEN_STAR,
  // One or two character tokens. 一或兩字符詞法
  TOKEN_BANG, TOKEN_BANG_EQUAL,
  TOKEN_EQUAL, TOKEN_EQUAL_EQUAL,
  TOKEN_GREATER, TOKEN_GREATER_EQUAL,
  TOKEN_LESS, TOKEN_LESS_EQUAL,
  // Literals. 字面量
  TOKEN_IDENTIFIER, TOKEN_STRING, TOKEN_NUMBER,
  // Keywords. 關鍵字
  TOKEN_AND, TOKEN_CLASS, TOKEN_ELSE, TOKEN_FALSE,
  TOKEN_FOR, TOKEN_FUN, TOKEN_IF, TOKEN_NIL, TOKEN_OR,
  TOKEN_PRINT, TOKEN_RETURN, TOKEN_SUPER, TOKEN_THIS,
  TOKEN_TRUE, TOKEN_VAR, TOKEN_WHILE,

  TOKEN_ERROR, TOKEN_EOF
} TokenType;

// 新增部分結束
typedef struct {
```

> Aside from prefixing all the names with `TOKEN_` (since C tosses enum names in the top-level namespace) the only difference is that extra `TOKEN_ERROR` type. What’s that about?

除了在所有名稱前都加上`TOKEN_`前綴（因為C語言會將枚舉名稱拋出到頂層命名空間）之外，唯一的區別就是多了一個`TOKEN_ERROR`類型。那是什麼呢？

> There are only a couple of errors that get detected during scanning: unterminated strings and unrecognized characters. In jlox, the scanner reports those itself. In clox, the scanner produces a synthetic “error” token for that error and passes it over to the compiler. This way, the compiler knows an error occurred and can kick off error recovery before reporting it.

在掃描過程中只會檢測到幾種錯誤：未終止的字符串和無法識別的字符。在jlox中，掃描器會自己報告這些錯誤。在clox中，掃描器會針對這些錯誤生成一個合成的“錯誤”標識，並將其傳遞給編譯器。這樣一來，編譯器就知道發生了一個錯誤，並可以在報告錯誤之前啓動錯誤恢復。

> The novel part in clox’s Token type is how it represents the lexeme. In jlox, each Token stored the lexeme as its own separate little Java string. If we did that for clox, we’d have to figure out how to manage the memory for those strings. That’s especially hard since we pass tokens by value—multiple tokens could point to the same lexeme string. Ownership gets weird.

在clox的Token類型中，新穎之處在於它如何表示一個詞素。在jlox中，每個Token將詞素保存到其單獨的Java字符串中。如果我們在clox中也這樣做，我們就必須想辦法管理這些字符串的內存。這非常困難，因為我們是通過值傳遞詞法標識的——多個標識可能指向相同的詞素字符串。所有權會變得混亂。

> Instead, we use the original source string as our character store. We represent a lexeme by a pointer to its first character and the number of characters it contains. This means we don’t need to worry about managing memory for lexemes at all and we can freely copy tokens around. As long as the main source code string outlives all of the tokens, everything works fine.
>

相反，我們將原始的源碼字符串作為我們的字符存儲。我們用指向第一個字符的指針和其中包含的字符數來表示一個詞素。這意味着我們完全不需要擔心管理詞素的內存，而且我們可以自由地複製詞法標識。只要主源碼字符串的壽命超過所有詞法標識，一切都可以正常工作[^5]。

> ### 16 . 2 . 1 Scanning tokens

### 16.2.1 掃描標識

> We’re ready to scan some tokens. We’ll work our way up to the complete implementation, starting with this:
>

我們已經準備好掃描一些標識了。我們將從下面的代碼開始，逐步達成完整的實現：

*<u>scanner.c，在initScanner()方法後添加代碼：</u>*

```c
Token scanToken() {
  scanner.start = scanner.current;

  if (isAtEnd()) return makeToken(TOKEN_EOF);

  return errorToken("Unexpected character.");
}
```

> Since each call to this function scans a complete token, we know we are at the beginning of a new token when we enter the function. Thus, we set `scanner.start` to point to the current character so we remember where the lexeme we’re about to scan starts.

由於對該函數的每次調用都會掃描一個完整的詞法標識，所以當我們進入該函數時，就知道我們正處於一個新詞法標識的開始處。因此，我們將`scanner.start`設置為指向當前字符，這樣我們就能記住我們將要掃描的詞素的開始位置。

> Then we check to see if we’ve reached the end of the source code. If so, we return an EOF token and stop. This is a sentinel value that signals to the compiler to stop asking for more tokens.

然後檢查是否已達到源代碼的結尾。如果是，我們返回一個EOF標識並停止。這是一個標記值，它向編譯器發出信號，停止請求更多標記。

> If we aren’t at the end, we do some . . . stuff . . . to scan the next token. But we haven’t written that code yet. We’ll get to that soon. If that code doesn’t successfully scan and return a token, then we reach the end of the function. That must mean we’re at a character that the scanner can’t recognize, so we return an error token for that.

如果我們沒有達到結尾，我們會做一些……事情……來掃描下一個標識。但我們還沒有寫這些代碼。我們很快就會講到。如果這段代碼沒有成功掃描並返回一個詞法標識，那麼我們就到達了函數的終點。這肯定意味着我們遇到了一個掃描器無法識別的字符，所以我們為此返回一個錯誤標識。

> This function relies on a couple of helpers, most of which are familiar from jlox. First up:

這個函數依賴於幾個輔助函數，其中大部分都是在jlox中已熟悉的。首先是：

*<u>scanner.c，在initScanner()方法後添加代碼：</u>*

```c
static bool isAtEnd() {
  return *scanner.current == '\0';
}
```

> We require the source string to be a good null-terminated C string. If the current character is the null byte, then we’ve reached the end.

我們要求源字符串是一個良好的以null結尾的C字符串。如果當前字符是null字節，那我們就到達了終點。

> To create a token, we have this constructor-like function:

要創建一個標識，我們還需要這個類似於構造函數的函數：

*<u>scanner.c，在isAtEnd()方法後添加代碼：</u>*

```c
static Token makeToken(TokenType type) {
  Token token;
  token.type = type;
  token.start = scanner.start;
  token.length = (int)(scanner.current - scanner.start);
  token.line = scanner.line;
  return token;
}
```

> It uses the scanner’s `start` and `current` pointers to capture the token’s lexeme. It sets a couple of other obvious fields then returns the token. It has a sister function for returning error tokens.

其中使用掃描器的`start`和`current`指針來捕獲標識的詞素。它還設置了其它幾個明顯的字段，如何返回標識。它還有一個用於返回錯誤標識的姊妹函數。

*<u>scanner.c，在makeToken()方法後添加代碼：</u>*

```c
static Token errorToken(const char* message) {
  Token token;
  token.type = TOKEN_ERROR;
  token.start = message;
  token.length = (int)strlen(message);
  token.line = scanner.line;
  return token;
}
```

> The only difference is that the “lexeme” points to the error message string instead of pointing into the user’s source code. Again, we need to ensure that the error message sticks around long enough for the compiler to read it. In practice, we only ever call this function with C string literals. Those are constant and eternal, so we’re fine.

唯一的區別在於，“詞素”指向錯誤信息字符串而不是用户的源代碼。同樣，我們需要確保錯誤信息能保持足夠長的時間，以便編譯器能夠讀取它。在實踐中，我們只會用C語言的字符串字面量來調用這個函數。它們是恆定不變的，所以我們不會有問題。

> What we have now is basically a working scanner for a language with an empty lexical grammar. Since the grammar has no productions, every character is an error. That’s not exactly a fun language to program in, so let’s fill in the rules.

我們現在所擁有的是一個基本可用的掃描器，用於掃描空語法語言。因為語法沒有產生式，所以每個字符都是一個錯誤。這並不是一種有趣的編程語言，所以讓我們把規則填進去。

> ## 16 . 3 A Lexical Grammar for Lox

## 16.3 Lox語法

> The simplest tokens are only a single character. We recognize those like so:

最簡單的詞法標識只有一個字符。我們這樣來識別它們：

*<u>scanner.c，在scanToken()方法中添加代碼：</u>*

```c
  if (isAtEnd()) return makeToken(TOKEN_EOF);
  // 新增部分開始
  char c = advance();

  switch (c) {
    case '(': return makeToken(TOKEN_LEFT_PAREN);
    case ')': return makeToken(TOKEN_RIGHT_PAREN);
    case '{': return makeToken(TOKEN_LEFT_BRACE);
    case '}': return makeToken(TOKEN_RIGHT_BRACE);
    case ';': return makeToken(TOKEN_SEMICOLON);
    case ',': return makeToken(TOKEN_COMMA);
    case '.': return makeToken(TOKEN_DOT);
    case '-': return makeToken(TOKEN_MINUS);
    case '+': return makeToken(TOKEN_PLUS);
    case '/': return makeToken(TOKEN_SLASH);
    case '*': return makeToken(TOKEN_STAR);
  }
  // 新增部分結束
  return errorToken("Unexpected character.");
```

> We read the next character from the source code, and then do a straightforward switch to see if it matches any of Lox’s one-character lexemes. To read the next character, we use a new helper which consumes the current character and returns it.

我們從源代碼中讀取下一個字符，然後做一個簡單的switch判斷，看它是否與Lox中的某個單字符詞素相匹配。為了讀取下一個字符，我們使用一個新的輔助函數，它會消費當前字符並將其返回。

*<u>scanner.c，在isAtEnd()方法後添加代碼：</u>*

```c
static char advance() {
  scanner.current++;
  return scanner.current[-1];
}
```

> Next up are the two-character punctuation tokens like `!=` and `>=`. Each of these also has a corresponding single-character token. That means that when we see a character like `!`, we don’t know if we’re in a `!` token or a `!=` until we look at the next character too. We handle those like so:

接下來是兩個字符的符號，如`!=`和`>=`，其中每一個都包含對應的單字符標識。這意味着，當我們看到一個像`!`這樣的字符時，我們只有看到下一個字符，才能確認當前是`!`標識還是`!=`標識。我們是這樣處理的：

*scanner.c*
in *scanToken*()

```c
    case '*': return makeToken(TOKEN_STAR);
    // 新增部分開始
    case '!':
      return makeToken(
          match('=') ? TOKEN_BANG_EQUAL : TOKEN_BANG);
    case '=':
      return makeToken(
          match('=') ? TOKEN_EQUAL_EQUAL : TOKEN_EQUAL);
    case '<':
      return makeToken(
          match('=') ? TOKEN_LESS_EQUAL : TOKEN_LESS);
    case '>':
      return makeToken(
          match('=') ? TOKEN_GREATER_EQUAL : TOKEN_GREATER);
    // 新增部分結束      
  }
```

> After consuming the first character, we look for an `=`. If found, we consume it and return the corresponding two-character token. Otherwise, we leave the current character alone (so it can be part of the *next* token) and return the appropriate one-character token.

在消費第一個字符之後，我們會嘗試尋找一個`=`。如果找到了，我們就消費它並返回對應的雙字符標識。否則，我們就不處理當前字符（這樣它就是下一個標識的一部分）並返回相應的單字符標識。

> That logic for conditionally consuming the second character lives here:

這個有條件地消費第二個字符的邏輯如下：

*<u>scanner.c，在advance()方法後添加：</u>*

```c
static bool match(char expected) {
  if (isAtEnd()) return false;
  if (*scanner.current != expected) return false;
  scanner.current++;
  return true;
}
```

> If the current character is the desired one, we advance and return `true`. Otherwise, we return `false` to indicate it wasn’t matched.

如果當前字符是所需的字符，則指針前進並返回`true`。否則，我們返回`false`表示沒有匹配。

> Now our scanner supports all of the punctuation-like tokens. Before we get to the longer ones, let’s take a little side trip to handle characters that aren’t part of a token at all.

現在我們的掃描器支持所有類似標點符號的標識。在我們處理更長的字符之前，我們先來處理一下那些根本不屬於標識的字符。

> ### 16 . 3 . 1 Whitespace

### 16.3.1 空白字符

> Our scanner needs to handle spaces, tabs, and newlines, but those characters don’t become part of any token’s lexeme. We could check for those inside the main character switch in `scanToken()` but it gets a little tricky to ensure that the function still correctly finds the next token *after* the whitespace when you call it. We’d have to wrap the whole body of the function in a loop or something.

我們的掃描器需要處理空格、製表符和換行符，但是這些字符不會成為任何標識詞素的一部分。我們可以在`scanToken()`中的主要的字符switch語句中檢查這些字符，但要想確保當你調用該函數時，它仍然能正確地找到空白字符後的下一個標識，這就有點棘手了。我們必須將整個函數封裝在一個循環或其它東西中。

> Instead, before starting the token, we shunt off to a separate function.

相應地，在開始掃描標識之前，我們切換到一個單獨的函數。

*<u>scanner.c，在scanToken()方法中添加代碼：</u>*

```c
Token scanToken() {
  // 新增部分開始
  skipWhitespace();
  // 新增部分結束
  scanner.start = scanner.current;
```

> This advances the scanner past any leading whitespace. After this call returns, we know the very next character is a meaningful one (or we’re at the end of the source code).

這將使掃描器跳過所有的前置空白字符。在這個調用返回後，我們知道下一個字符是一個有意義的字符（或者我們到達了源代碼的末尾）。

*<u>scanner.c，在errorToken()方法後添加：</u>*

```c
static void skipWhitespace() {
  for (;;) {
    char c = peek();
    switch (c) {
      case ' ':
      case '\r':
      case '\t':
        advance();
        break;
      default:
        return;
    }
  }
}
```

> It’s sort of a separate mini-scanner. It loops, consuming every whitespace character it encounters. We need to be careful that it does *not* consume any *non*-whitespace characters. To support that, we use this:

這有點像一個獨立的微型掃描器。它循環，消費遇到的每一個空白字符。我們需要注意的是，它*不會*消耗任何非空白字符。為了支持這一點，我們使用下面的函數：

*<u>scanner.c，在 advance()方法後添加代碼：</u>*

```c
static char peek() {
  return *scanner.current;
}
```

> This simply returns the current character, but doesn’t consume it. The previous code handles all the whitespace characters except for newlines.

這只是簡單地返回當前字符，但並不消費它。前面的代碼已經處理了除換行符外的所有空白字符。

*<u>scanner.c，在skipWhitespace()方法內添加代碼：</u>*

```c
        break;
      // 新增部分開始
      case '\n':
        scanner.line++;
        advance();
        break;
      // 新增部分結束  
      default:
        return;
```

> When we consume one of those, we also bump the current line number.

當我們消費換行符時，也會增加當前行數。

> ### 16 . 3 . 2 Comments

### 16.3.2 註釋

> Comments aren’t technically “whitespace”, if you want to get all precise with your terminology, but as far as Lox is concerned, they may as well be, so we skip those too.

如果你想用精確的術語，那註釋在技術上來説不是“空白字符”，但就Lox目前而言，它們也可以是，所以我們也跳過它們。

*<u>scanner.c，在skipWhitespace()函數內添加代碼：</u>*

```c
        break;
      // 新增部分開始  
      case '/':
        if (peekNext() == '/') {
          // A comment goes until the end of the line.
          while (peek() != '\n' && !isAtEnd()) advance();
        } else {
          return;
        }
        break;
      // 新增部分結束  
      default:
        return;
```

> Comments start with `//` in Lox, so as with `!=` and friends, we need a second character of lookahead. However, with `!=`, we still wanted to consume the `!` even if the `=` wasn’t found. Comments are different. If we don’t find a second `/`, then `skipWhitespace()` needs to not consume the *first* slash either.

Lox中的註釋以`//`開頭，因此與`!=`類似，我們需要前瞻第二個字符。然而，在處理`!=`時，即使沒有找到`=`，也仍然希望消費`!`。註釋是不同的。如果我們沒有找到第二個`/`，那麼`skipWhitespace()`也不需要消費第一個斜槓。

> To handle that, we add:

為此，我們添加以下函數：

*<u>scanner.c，在peek()方法後添加代碼：</u>*

```c
static char peekNext() {
  if (isAtEnd()) return '\0';
  return scanner.current[1];
}
```

> This is like `peek()` but for one character past the current one. If the current character and the next one are both `/`, we consume them and then any other characters until the next newline or the end of the source code.

這就像`peek()`一樣，但是是針對當前字符之後的一個字符。如果當前字符和下一個字符都是`/`，則消費它們，然後再消費其它字符，直到遇見下一個換行符或源代碼結束。

> We use `peek()` to check for the newline but not consume it. That way, the newline will be the current character on the next turn of the outer loop in `skipWhitespace()` and we’ll recognize it and increment `scanner.line`.

我們使用`peek()`來檢查換行符，但是不消費它。這樣一來，換行符將成為`skipWhitespace()`外部下一輪循環中的當前字符，我們就能識別它並增加`scanner.line`。

> ### 16 . 3 . 3 Literal tokens

### 16.3.3 字面量標識

> Number and string tokens are special because they have a runtime value associated with them. We’ll start with strings because they are easy to recognize—they always begin with a double quote.

數字和字符串標識比較特殊，因為它們有一個與之關聯的運行時值。我們會從字符串開始，因為它們很容易識別——總是以雙引號開始。

*<u>scanner.c，在 scanToken()方法中添加代碼：</u>*

```c
          match('=') ? TOKEN_GREATER_EQUAL : TOKEN_GREATER);
    // 新增部分開始
    case '"': return string();
    // 新增部分結束
  }
```

> That calls a new function.

它會調用一個新函數：

*<u>scanner.c，在 skipWhitespace()方法後添加代碼：</u>*

```c
static Token string() {
  while (peek() != '"' && !isAtEnd()) {
    if (peek() == '\n') scanner.line++;
    advance();
  }

  if (isAtEnd()) return errorToken("Unterminated string.");

  // The closing quote.
  advance();
  return makeToken(TOKEN_STRING);
}
```

> Similar to jlox, we consume characters until we reach the closing quote. We also track newlines inside the string literal. (Lox supports multi-line strings.) And, as ever, we gracefully handle running out of source code before we find the end quote.

與jlox類似，我們消費字符，直到遇見右引號。我們也會追蹤字符串字面量中的換行符（Lox支持多行字符串）。並且，與之前一樣，我們會優雅地處理在找到結束引號之前源代碼耗盡的問題。

> The main change here in clox is something that’s *not* present. Again, it relates to memory management. In jlox, the Token class had a field of type Object to store the runtime value converted from the literal token’s lexeme.

clox中的主要變化是一些不存在的東西。同樣，這與內存管理有關。在jlox中，Token類有一個Object類型的字段，用於存儲從字面量詞素轉換而來的運行時值。

> Implementing that in C would require a lot of work. We’d need some sort of union and type tag to tell whether the token contains a string or double value. If it’s a string, we’d need to manage the memory for the string’s character array somehow.

在C語言中實現這一點需要大量的工作。我們需要某種union和type標籤來告訴我們標識中是否包含字符串或浮點數。如果是字符串，我們還需要以某種方式管理字符串中字符數組的內存。

> Instead of adding that complexity to the scanner, we defer converting the literal lexeme to a runtime value until later. In clox, tokens only store the lexeme—the character sequence exactly as it appears in the user’s source code. Later in the compiler, we’ll convert that lexeme to a runtime value right when we are ready to store it in the chunk’s constant table.

我們沒有給掃描器增加這種複雜性，我們把字面量詞素轉換為運行值的工作推遲到以後。在clox中，詞法標識只存儲詞素——即用户源代碼中出現的字符序列。稍後在編譯器中，當我們準備將其存儲在字節碼塊中的常量表中時，我們會將詞素轉換為運行時值[^6]。

> Next up, numbers. Instead of adding a switch case for each of the ten digits that can start a number, we handle them here:

接下來是數字。我們沒有為可能作為數字開頭的10個數位各添加對應的switch分支，而是使用如下方式處理：

*<u>scanner.c，在scanToken()方法中添加代碼：</u>*

```c
  char c = advance();
  // 新增部分開始
  if (isDigit(c)) return number();
  // 新增部分結束
  switch (c) {
```

> That uses this obvious utility function:

這裏使用了下面這個明顯的工具函數：

*<u>scanner.c，在initScanner()方法後添加代碼：</u>*

```c
static bool isDigit(char c) {
  return c >= '0' && c <= '9';
}
```

> We finish scanning the number using this:

我們使用下面的函數完成掃描數字的工作：

*<u>scanner.c，在skipWhitespace()方法後添加代碼：</u>*

```c
static Token number() {
  while (isDigit(peek())) advance();

  // Look for a fractional part.
  if (peek() == '.' && isDigit(peekNext())) {
    // Consume the ".".
    advance();

    while (isDigit(peek())) advance();
  }

  return makeToken(TOKEN_NUMBER);
}
```

> It’s virtually identical to jlox’s version except, again, we don’t convert the lexeme to a double yet.

它與jlox版本幾乎是相同的，只是我們還沒有將詞素轉換為浮點數。

> ## 16 . 4 Identifiers and Keywords
>

## 16.4 標識符和關鍵字

> The last batch of tokens are identifiers, both user-defined and reserved. This section should be fun—the way we recognize keywords in clox is quite different from how we did it in jlox, and touches on some important data structures.

最後一批詞法是標識符，包括用户定義的和保留字。這一部分應該很有趣——我們在clox中識別關鍵字的方式與我們在jlox中的方式完全不同，而且涉及到一些重要的數據結構。

> First, though, we have to scan the lexeme. Names start with a letter or underscore.

不過，首先我們需要掃描詞素。名稱以字母或下劃線開頭。

*<u>scanner.c，在scanToken()方法中添加代碼：</u>*

```c
  char c = advance();
  // 新增部分開始
  if (isAlpha(c)) return identifier();
  // 新增部分結束
  if (isDigit(c)) return number();
```

> We recognize those using this:

我們使用這個方法識別這些標識符：

*<u>scanner.c，在initScanner()方法後添加代碼：</u>*

```c
static bool isAlpha(char c) {
  return (c >= 'a' && c <= 'z') ||
         (c >= 'A' && c <= 'Z') ||
          c == '_';
}
```

> Once we’ve found an identifier, we scan the rest of it here:

一旦我們發現一個標識符，我們就通過下面的方法掃描其餘部分：

*<u>scanner.c，在skipWhitespace()方法後添加代碼：</u>*

```c
static Token identifier() {
  while (isAlpha(peek()) || isDigit(peek())) advance();
  return makeToken(identifierType());
}
```

> After the first letter, we allow digits too, and we keep consuming alphanumerics until we run out of them. Then we produce a token with the proper type. Determining that “proper” type is the unique part of this chapter.

在第一個字母之後，我們也允許使用數字，並且我們會一直消費字母數字，直到消費完為止。然後我們生成一個具有適當類型的詞法標識。確定“適當”類型是本章的特點部分。

*<u>scanner.c，在skipWhitespace()方法後添加代碼：</u>*

```c
static TokenType identifierType() {
  return TOKEN_IDENTIFIER;
}
```

> Okay, I guess that’s not very exciting yet. That’s what it looks like if we have no reserved words at all. How should we go about recognizing keywords? In jlox, we stuffed them all in a Java Map and looked them up by name. We don’t have any sort of hash table structure in clox, at least not yet.

好吧，我想這還不算很令人興奮。如果我們沒有保留字，那就是這個樣子了。我們應該如何去識別關鍵字呢？在jlox中，我們將其都塞入一個Java Map中，然後按名稱查找它們。在clox中，我們沒有任何類型的哈希表結構，至少現在還沒有。

> A hash table would be overkill anyway. To look up a string in a hash table, we need to walk the string to calculate its hash code, find the corresponding bucket in the hash table, and then do a character-by-character equality comparison on any string it happens to find there.

無論如何，哈希表都是冗餘的。要在哈希表中查找一個字符串，我們需要遍歷該字符串以計算其哈希碼，在哈希表中找到對應的桶，然後對其中的所有字符串逐個字符進行相等比較[^7]。

> Let’s say we’ve scanned the identifier “gorgonzola”. How much work *should* we need to do to tell if that’s a reserved word? Well, no Lox keyword starts with “g”, so looking at the first character is enough to definitively answer no. That’s a lot simpler than a hash table lookup.

假定我們已經掃描到了標識符“gorgonzola”。我們需要做多少工作來判斷這是否是一個保留字？好吧，沒有Lox關鍵字是以“g”開頭的，所以看第一個字符就足以明確地回答不是。這比哈希表查詢要簡單的多。

> What about “cardigan”? We do have a keyword in Lox that starts with “c”: “class”. But the second character in “cardigan”, “a”, rules that out. What about “forest”? Since “for” is a keyword, we have to go farther in the string before we can establish that we don’t have a reserved word. But, in most cases, only a character or two is enough to tell we’ve got a user-defined name on our hands. We should be able to recognize that and fail fast.

那“cardigan”呢？我們在Lox中確實有一個以“c”開頭的關鍵字：“class”。但是“cardigan”中的第二個字符“a”就排除了這種情況。那“forest”呢？因為“for”是一個關鍵字，我們必須在字符串中繼續遍歷，才能確定這不是一個保留字。但是，在大多數情況下，只有一兩個字符就足以告訴我們現在處理的是一個用户定義的名稱。我們應該能夠意識到這一點，並快速失敗。

> Here’s a visual representation of that branching character-inspection logic:

下面是這個分支字符檢查邏輯的一個可視化表示[^8]：

![A trie that contains all of Lox's keywords.](16.按需掃描/keywords.png)

> We start at the root node. If there is a child node whose letter matches the first character in the lexeme, we move to that node. Then repeat for the next letter in the lexeme and so on. If at any point the next letter in the lexeme doesn’t match a child node, then the identifier must not be a keyword and we stop. If we reach a double-lined box, and we’re at the last character of the lexeme, then we found a keyword.

我們從根節點開始。如果有一個子節點的字母與詞素中的第一個字符相匹配，我們就移動到該節點上。然後對詞素中的下一個字母重複此操作，以此類推。如果在任意節點上，詞素的下一個字符沒有匹配到子節點，那麼該標識符一定不是一個關鍵字，我們就停止。如果我們到達了一個雙線框，並且我們在詞素的最後一個字符處，那麼我們就找到了一個關鍵字。

> ### 16 . 4 . 1 Tries and state machines
>

### 16.4.1 字典樹和狀態機

> This tree diagram is an example of a thing called a [**trie**](https://en.wikipedia.org/wiki/Trie). A trie stores a set of strings. Most other data structures for storing strings contain the raw character arrays and then wrap them inside some larger construct that helps you search faster. A trie is different. Nowhere in the trie will you find a whole string.

這個樹狀圖是[**trie**](https://en.wikipedia.org/wiki/Trie)[^9]（字典樹）的一個例子。字典樹會存儲一組字符串。大多數其它用於存儲字符串的數據結構都包含原始字符數組，然後將它們封裝在一些更大的結果中，以幫助你更快地搜索。字典樹則不同，在其中你找不到一個完整的字符串。

> Instead, each string the trie “contains” is represented as a *path* through the tree of character nodes, as in our traversal above. Nodes that match the last character in a string have a special marker—the double lined boxes in the illustration. That way, if your trie contains, say, “banquet” and “ban”, you are able to tell that it does *not* contain “banque”—the “e” node won’t have that marker, while the “n” and “t” nodes will.

相應地，字典樹中“包含”的每個字符串被表示為通過字符樹中節點的路徑，就像上面的遍歷一樣。用於匹配字符串中最後一個字符的節點中有一個特殊的標記——插圖中的雙線框。這樣一來，假定你的字典樹中包含“banquet”和“ban”，你就能知道它不包括“banque”——“e”節點沒有這個標記，而“n”和“t”節點中有。

> Tries are a special case of an even more fundamental data structure: a [**deterministic finite automaton**](https://en.wikipedia.org/wiki/Deterministic_finite_automaton) (**DFA**). You might also know these by other names: **finite state machine**, or just **state machine**. State machines are rad. They end up useful in everything from [game programming](http://gameprogrammingpatterns.com/state.html) to implementing networking protocols.

字典樹是一種更基本的數據結構的特殊情況：確定性有限狀態機（[**deterministic finite automaton**](https://en.wikipedia.org/wiki/Deterministic_finite_automaton) ，**DFA**）。你可能還知道它的其它名字：**有限狀態機**，或就叫**狀態機**。狀態機是非常重要的，從[遊戲編程](http://gameprogrammingpatterns.com/state.html)到實現網絡協議的一切方面都很有用。

> In a DFA, you have a set of *states* with *transitions* between them, forming a graph. At any point in time, the machine is “in” exactly one state. It gets to other states by following transitions. When you use a DFA for lexical analysis, each transition is a character that gets matched from the string. Each state represents a set of allowed characters.

在DFA中，你有一組*狀態*，它們之間有*轉換*，形成一個圖。在任何時間點，機器都“處於”其中一個狀態。它通過轉換過渡到其它狀態。當你使用DFA進行詞法分析時，每個轉換都是從字符串中匹配到的一個字符。每個狀態代表一組允許的字符。

> Our keyword tree is exactly a DFA that recognizes Lox keywords. But DFAs are more powerful than simple trees because they can be arbitrary *graphs*. Transitions can form cycles between states. That lets you recognize arbitrarily long strings. For example, here’s a DFA that recognizes number literals:

我們的關鍵字樹正是一個能夠識別Lox關鍵字的DFA。但是DFA比簡單的樹更強大，因為它們可以是任意的圖。轉換可以在狀態之間形成循環。這讓你可以識別任意長的字符串。舉例來説，下面是一個可以識別數字字面量的DFA[^10]：

![A syntax diagram that recognizes integer and floating point literals.](16.按需掃描/numbers.png)

> I’ve collapsed the nodes for the ten digits together to keep it more readable, but the basic process works the same—you work through the path, entering nodes whenever you consume a corresponding character in the lexeme. If we were so inclined, we could construct one big giant DFA that does *all* of the lexical analysis for Lox, a single state machine that recognizes and spits out all of the tokens we need.

我把十個數位的節點摺疊在一起，以使其更易於閲讀，但是基本的過程是相同的——遍歷路徑，每當你消費詞素中的一個字符，就進入對應節點。如果我們願意的話，可以構建一個巨大的DFA來完成Lox的所有詞法分析，用一個狀態機來識別並輸出我們需要的所有詞法標識。

> However, crafting that mega-DFA by hand would be challenging. That’s why [Lex](https://en.wikipedia.org/wiki/Lex_(software)) was created. You give it a simple textual description of your lexical grammar—a bunch of regular expressions—and it automatically generates a DFA for you and produces a pile of C code that implements it.

然而，手工完成這種巨型DFA是一個巨大的挑戰。這就是[Lex](https://en.wikipedia.org/wiki/Lex_(software))誕生的原因。你給它一個關於語法的簡單文本描述——一堆正則表達式——它就會自動為你生成一個DFA，並生成一堆實現它的C代碼[^11]。

This is also how most regular expression engines in programming languages and text editors work under the hood. They take your regex string and convert it to a DFA, which they then use to match strings.

If you want to learn the algorithm to convert a regular expression into a DFA, [the dragon book](https://en.wikipedia.org/wiki/Compilers:_Principles,_Techniques,_and_Tools) has you covered.

> We won’t go down that road. We already have a perfectly serviceable hand-rolled scanner. We just need a tiny trie for recognizing keywords. How should we map that to code?

我們就不走這條路了。我們已經有了一個完全可用的簡單掃描器。我們只需要一個很小的字典樹來識別關鍵字。我們應該如何將其映射到代碼中？

> The absolute simplest solution is to use a switch statement for each node with cases for each branch. We’ll start with the root node and handle the easy keywords.

最簡單的解決方案是對每個節點使用一個switch語句，每個分支是一個case。我們從根節點開始，處理簡單的關鍵字[^12]。

*<u>scanner.c，在identifierType()方法中添加代碼：</u>*

```c
static TokenType identifierType() {
  // 新增部分開始
  switch (scanner.start[0]) {
    case 'a': return checkKeyword(1, 2, "nd", TOKEN_AND);
    case 'c': return checkKeyword(1, 4, "lass", TOKEN_CLASS);
    case 'e': return checkKeyword(1, 3, "lse", TOKEN_ELSE);
    case 'i': return checkKeyword(1, 1, "f", TOKEN_IF);
    case 'n': return checkKeyword(1, 2, "il", TOKEN_NIL);
    case 'o': return checkKeyword(1, 1, "r", TOKEN_OR);
    case 'p': return checkKeyword(1, 4, "rint", TOKEN_PRINT);
    case 'r': return checkKeyword(1, 5, "eturn", TOKEN_RETURN);
    case 's': return checkKeyword(1, 4, "uper", TOKEN_SUPER);
    case 'v': return checkKeyword(1, 2, "ar", TOKEN_VAR);
    case 'w': return checkKeyword(1, 4, "hile", TOKEN_WHILE);
  }
  // 新增部分結束
  return TOKEN_IDENTIFIER;
```

> These are the initial letters that correspond to a single keyword. If we see an “s”, the only keyword the identifier could possibly be is `super`. It might not be, though, so we still need to check the rest of the letters too. In the tree diagram, this is basically that straight path hanging off the “s”.

這些是對應於單個關鍵字的首字母。如果我們看到一個“s”，那麼這個標識符唯一可能的關鍵字就是`super`。但也可能不是，所以我們仍然需要檢查其餘的字母。在樹狀圖中，這基本上就是掛在“s”上的一條直線路徑。

> We won’t roll a switch for each of those nodes. Instead, we have a utility function that tests the rest of a potential keyword’s lexeme.

我們不會為每個節點都增加一個switch語句。相反，我們有一個工具函數來測試潛在關鍵字詞素的剩餘部分。

*<u>scanner.c，在skipWhitespace()方法後添加代碼：</u>*

```c
static TokenType checkKeyword(int start, int length,
    const char* rest, TokenType type) {
  if (scanner.current - scanner.start == start + length &&
      memcmp(scanner.start + start, rest, length) == 0) {
    return type;
  }

  return TOKEN_IDENTIFIER;
}
```

> We use this for all of the unbranching paths in the tree. Once we’ve found a prefix that could only be one possible reserved word, we need to verify two things. The lexeme must be exactly as long as the keyword. If the first letter is “s”, the lexeme could still be “sup” or “superb”. And the remaining characters must match exactly—“supar” isn’t good enough.

我們將此用於樹中的所有無分支路徑。一旦我們發現一個前綴，其只有可能是一種保留字，我們需要驗證兩件事。詞素必須與關鍵字一樣長。如果第一個字母是“s”，詞素仍然可以是“sup”或“superb”。剩下的字符必須完全匹配——“supar”就不夠好。

> If we do have the right number of characters, and they’re the ones we want, then it’s a keyword, and we return the associated token type. Otherwise, it must be a normal identifier.

如果我們字符數量確實正確，並且它們是我們想要的字符，那這就是一個關鍵字，我們返回相關的標識類型。否則，它必然是一個普通的標識符。

> We have a couple of keywords where the tree branches again after the first letter. If the lexeme starts with “f”, it could be `false`, `for`, or `fun`. So we add another switch for the branches coming off the “f” node.

我們有幾個關鍵字是在第一個字母之後又有樹的分支。如果詞素以“f”開頭，它可能是`false`、`for`或`fun`。因此我們在“f”節點下的分支中添加一個switch語句。

*<u>scanner.c，在identifierType()方法中添加代碼：</u>*

```c
    case 'e': return checkKeyword(1, 3, "lse", TOKEN_ELSE);
    // 新增部分開始
    case 'f':
      if (scanner.current - scanner.start > 1) {
        switch (scanner.start[1]) {
          case 'a': return checkKeyword(2, 3, "lse", TOKEN_FALSE);
          case 'o': return checkKeyword(2, 1, "r", TOKEN_FOR);
          case 'u': return checkKeyword(2, 1, "n", TOKEN_FUN);
        }
      }
      break;
    // 新增部分結束  
    case 'i': return checkKeyword(1, 1, "f", TOKEN_IF);
```

> Before we switch, we need to check that there even *is* a second letter. “f” by itself is a valid identifier too, after all. The other letter that branches is “t”.

在我們進入switch語句之前，需要先檢查是否有第二個字母。畢竟，“f”本身也是一個有效的標識符。另外一個需要分支的字母是“t”。

*<u>scanner.c，在identifierType()方法中添加代碼：</u>*

```c
    case 's': return checkKeyword(1, 4, "uper", TOKEN_SUPER);
    // 新增部分開始
    case 't':
      if (scanner.current - scanner.start > 1) {
        switch (scanner.start[1]) {
          case 'h': return checkKeyword(2, 2, "is", TOKEN_THIS);
          case 'r': return checkKeyword(2, 2, "ue", TOKEN_TRUE);
        }
      }
      break;
    // 新增部分結束  
    case 'v': return checkKeyword(1, 2, "ar", TOKEN_VAR);
```

> That’s it. A couple of nested `switch` statements. Not only is this code short, but it’s very, very fast. It does the minimum amount of work required to detect a keyword, and bails out as soon as it can tell the identifier will not be a reserved one.

就是這樣。幾個嵌套的`switch`語句。這段代碼不僅短，而且非常非常快。它只做了檢測一個關鍵字所需的最少的工作，而且一旦知道這個標識符不是一個保留字，就會直接結束[^13]。

> And with that, our scanner is complete.

這樣一來，我們的掃描器就完整了。



[^1]: 代碼裏面校驗是一個參數還是兩個參數，而不是0和1，因為argv中的第一個參數總是被運行的可執行文件的名稱。
[^2]: C語言不僅要求我們顯式地管理內存，而且要在精神上管理。我們程序員必須記住所有權規則，並在整個程序中手動實現。Java為我們做了這些。C++為我們提供了直接編碼策略的工具，這樣編譯器就會為我們驗證它。我喜歡C語言的簡潔，但是我們為此付出了真正的代價——這門語言要求我們更加認真。
[^3]: 嗯，這個size要加1，永遠記得為null字節留出空間。
[^4]: 格式字符串中的`%.*s`是一個很好的特性。通常情況下，你需要在格式字符串中寫入一個數字來設置輸出精度——要顯示的字符數。使用`*`則可以把精度作為一個參數來傳遞。因此，`printf()`調用將字符串從`token.start`開始的前`token.length`個字符。我們需要這樣限制長度，因為詞素指向原始的源碼字符串，並且在末尾沒有終止符。
[^5]: 我並不想讓這個聽起來太輕率。我們確實需要考慮並確保在“main”模塊中創建的源字符串具有足夠長的生命週期。這就是`runFile()`中會在`interpret()`執行完代碼並返回後才釋放字符串的原因。
[^6]: 在編譯器中進行詞素到運行時值的轉換確實會引入一些冗餘。掃描一個數字字面量的工作與將一串數字字符轉換為一個數值的工作非常相似。但是並沒有那麼多冗餘，它並不是任何性能上的關鍵點，而且能使得我們的掃描器更加簡單。
[^7]: 如果你對此不熟悉，請不要擔心。當我們從頭開始構建我們自己的哈希表時，將會學習關於它的所有細節。
[^8]: 從上向下閲讀每個節點鏈，你將看到Lox的關鍵字。
[^9]: “Trie”是CS中最令人困惑的名字之一。Edward Fredkin從“檢索（retrieval）”中把這個詞提取出來，這意味着它的讀音應該像“tree”。但是，已經有一個非常重要的數據結構發音為“tree”，而trie只是一個特例。所以如果你談論這些東西時，沒人能分辨出你在説哪一個。因此，現在人們經常把它讀作“try”，以免頭痛。
[^10]: 這種風格的圖被稱為[語法圖](https://en.wikipedia.org/wiki/Syntax_diagram)或**鐵路圖**。後者的名字是因為它看起來像火車的調度場。<br>在Backus-Naur範式出現之前，這是記錄語言語法的主要方式之一。如今，我們大多使用文本，但一種*文本語言*的官方規範依賴於*圖像*，這一點很令人高興。
[^11]: 這也是大多數編程語言和文本編輯器中的正則表達式引擎的工作原理。它們獲取你的正則表達式字符串並將其轉換為DFA，然後使用DFA來匹配字符串。<br>如果你想學習將正則表達式轉換為DFA的算法，[龍書](https://en.wikipedia.org/wiki/Compilers:_Principles,_Techniques,_and_Tools)中已經為你提供了答案。
[^12]: 簡單並不意味着愚蠢。V8也採用了同樣的方法，而它是目前世界上最複雜、最快的語言實現之一。
[^13]: 我們有時會陷入這樣的誤區：任務性能來自於複雜的數據結構、多級緩存和其它花哨的優化。但是，很多時候所需要的就是做更少的工作，而我經常發現，編寫最簡單的代碼就足以完成這些工作。

---

> ## CHALLENGES

## 習題

1. > Many newer languages support [**string interpolation**](https://en.wikipedia.org/wiki/String_interpolation). Inside a string literal, you have some sort of special delimiters—most commonly `${` at the beginning and `}` at the end. Between those delimiters, any expression can appear. When the string literal is executed, the inner expression is evaluated, converted to a string, and then merged with the surrounding string literal.

   許多較新的語言都支持[字符串插值](https://en.wikipedia.org/wiki/String_interpolation)。在字符串字面量中，有一些特殊的分隔符——最常見的是以`${`開頭以`}`結尾。在這些分隔符之間，可以出現任何表達式。當字符串字面量被執行時，內部表達式也會求值，轉換為字符串，然後與周圍的字符串字面量合併。

   > For example, if Lox supported string interpolation, then this . . . 

   舉例來説，如果Lox支持字符串插值，那麼下面的代碼……

   ```c
   var drink = "Tea";
   var steep = 4;
   var cool = 2;
   print "${drink} will be ready in ${steep + cool} minutes.";
   ```

   >  . . . would print:

   將會輸出：

   ```
   Tea will be ready in 6 minutes.
   ```

   > What token types would you define to implement a scanner for string interpolation? What sequence of tokens would you emit for the above string literal?

   你會定義什麼標識類型來實現支持字符串插值的掃描器？對於上面的字符串，你會生成什麼樣的標識序列？

   > What tokens would you emit for:

   下面的字符串會產生哪些標識：

   ```
   "Nested ${"interpolation?! Are you ${"mad?!"}"}"
   ```

   > Consider looking at other language implementations that support interpolation to see how they handle it.

   可以考慮看看其它支持插值的語言實現，看它們是如何處理的。

   

2. > Several languages use angle brackets for generics and also have a `>>` right shift operator. This led to a classic problem in early versions of C++:

   有些語言使用尖括號來表示泛型，也有右移操作符`>>`。這就導致了C++早期版本中的一個經典問題：

   ```c
   vector<vector<string>> nestedVectors;
   ```

   > This would produce a compile error because the `>>` was lexed to a single right shift token, not two `>` tokens. Users were forced to avoid this by putting a space between the closing angle brackets.

   這將產生一個編譯錯誤，因為`>>`被詞法識別為一個右移符號，而不是兩個`>`標識。用户不得不在右側的兩個尖括號之間增加一個空格來避免這種情況。

   > Later versions of C++ are smarter and can handle the above code. Java and C# never had the problem. How do those languages specify and implement this?

   後續的C++版本更加智能，可以處理上述代碼。Java和C#從未出現過這個問題。這些語言是如何規定和實現這一點的呢？

3. > Many languages, especially later in their evolution, define “contextual keywords”. These are identifiers that act like reserved words in some contexts but can be normal user-defined identifiers in others.

   許多語言，尤其是在其發展的後期，都定義了“上下文式關鍵字”。這些標識符在某些情況下類似於保留字，但在其它上下文中可以是普通的用户定義的標識符。

   > For example, `await` is a keyword inside an `async` method in C#, but in other methods, you can use `await` as your own identifier.

   例如，在C#中，`await`在`async`方法中是一個關鍵字，但在其它方法中，你可以使用`await`作為自己的標識符。

   > Name a few contextual keywords from other languages, and the context where they are meaningful. What are the pros and cons of having contextual keywords? How would you implement them in your language’s front end if you needed to?

   説出幾個來自其它語言中的上下文關鍵字，以及它們在哪些情況下是有意義的。擁有上下文關鍵字的優點和缺點是什麼？如果需要的話，你要如何在語言的前端中實現它們？
